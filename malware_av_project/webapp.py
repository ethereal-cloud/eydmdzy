from __future__ import annotations

import csv
import os
import shutil
import sys
import tempfile
import json
import yaml
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Iterator, List, Tuple
from uuid import uuid4

from flask import Flask, abort, redirect, render_template, request, send_file, url_for

from avscan.metrics import compute_confusion
from avscan.utils import iter_files, sha256_file
from avscan.yara_scanner import YaraEngine
from avscan.zircolite_runner import find_zircolite_path, parse_zircolite_hits, run_zircolite

BASE_DIR = Path(__file__).resolve().parent
OUTPUT_DIR = BASE_DIR / "outputs"

DEFAULT_SCAN_FORM = {
    "timeout": "20",
    "hash": True,
}

FILE_LIMIT = 200
SIGMA_LIMIT = 100

app = Flask(__name__)
_SCAN_CACHE: Dict[str, Dict[str, Any]] = {}
_LATEST_SCAN_ID: str | None = None


@app.after_request
def _no_cache(response):
    response.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
    response.headers["Pragma"] = "no-cache"
    response.headers["Expires"] = "0"
    return response


def _parse_bool(value: str | None) -> bool:
    return str(value).lower() in {"1", "true", "yes", "on"}


def _safe_int(value: str | None, default: int) -> int:
    try:
        return int(str(value))
    except Exception:
        return default


def _job_id() -> str:
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"{stamp}_{time.time_ns() % 100000:05d}"


def _prune_reports(keep: int = 5) -> None:
    if keep <= 0 or not OUTPUT_DIR.exists():
        return
    reports = sorted(
        OUTPUT_DIR.glob("web_report_*.json"),
        key=lambda p: p.stat().st_mtime,
        reverse=True,
    )
    for old in reports[keep:]:
        try:
            old.unlink()
        except Exception:
            continue


def _sanitize_relpath(name: str) -> Path:
    raw = Path(name)
    parts: List[str] = []
    for part in raw.parts:
        if part in ("", ".", ".."):
            continue
        if ":" in part:
            continue
        part = part.strip("/\\")
        if part:
            parts.append(part)
    return Path(*parts) if parts else Path(raw.name)


def _save_uploads(files: List[Any], dest_dir: Path) -> List[Path]:
    saved: List[Path] = []
    for f in files:
        if not f or not getattr(f, "filename", ""):
            continue
        rel = _sanitize_relpath(f.filename)
        target = dest_dir / rel
        target.parent.mkdir(parents=True, exist_ok=True)
        f.save(str(target))
        saved.append(target)
    return saved


def _iter_json_events(path: Path) -> Iterator[Dict[str, Any]]:
    suffix = path.suffix.lower()
    if suffix in {".jsonl", ".ndjson"}:
        with path.open("r", encoding="utf-8", errors="ignore") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    evt = json.loads(line)
                except Exception:
                    continue
                if isinstance(evt, dict):
                    yield evt
    elif suffix == ".json":
        try:
            data = json.loads(path.read_text(encoding="utf-8", errors="ignore"))
        except Exception:
            return
        if isinstance(data, list):
            for item in data:
                if isinstance(item, dict):
                    yield item
        elif isinstance(data, dict):
            yield data


def _parse_evtx_xml(xml: str) -> Dict[str, Any]:
    import xml.etree.ElementTree as ET

    try:
        root = ET.fromstring(xml)
    except Exception:
        return {}

    ns = {"e": "http://schemas.microsoft.com/win/2004/08/events/event"}

    def find_text(path: str) -> str | None:
        node = root.find(path, ns)
        return node.text if node is not None else None

    def find_attr(path: str, attr: str) -> str | None:
        node = root.find(path, ns)
        return node.get(attr) if node is not None else None

    event_id = find_text("./e:System/e:EventID")
    try:
        event_id_val: Any = int(event_id) if event_id and event_id.isdigit() else event_id
    except Exception:
        event_id_val = event_id

    system: Dict[str, Any] = {
        "EventID": event_id_val,
        "ProviderName": find_attr("./e:System/e:Provider", "Name"),
        "Channel": find_text("./e:System/e:Channel"),
        "Computer": find_text("./e:System/e:Computer"),
        "TimeCreated": find_attr("./e:System/e:TimeCreated", "SystemTime"),
        "EventRecordID": find_text("./e:System/e:EventRecordID"),
        "Task": find_text("./e:System/e:Task"),
        "Level": find_text("./e:System/e:Level"),
    }

    event_data: Dict[str, Any] = {}
    for node in root.findall("./e:EventData/e:Data", ns):
        name = node.get("Name") or "Data"
        value = node.text or ""
        if name in event_data:
            prev = event_data[name]
            if isinstance(prev, list):
                prev.append(value)
            else:
                event_data[name] = [prev, value]
        else:
            event_data[name] = value

    event: Dict[str, Any] = {
        "System": {k: v for k, v in system.items() if v is not None},
        "EventData": event_data,
    }

    for k, v in system.items():
        if v is not None:
            event[k] = v
    for k, v in event_data.items():
        event[k] = v

    return event


def _iter_evtx_events(path: Path) -> Iterator[Dict[str, Any]]:
    try:
        from Evtx.Evtx import Evtx
    except Exception as exc:
        raise RuntimeError("python-evtx is required to parse .evtx logs.") from exc

    with Evtx(str(path)) as evtx:
        for record in evtx.records():
            event = _parse_evtx_xml(record.xml())
            if event:
                yield event


def _iter_sigma_events(path: Path) -> Iterator[Dict[str, Any]]:
    suffix = path.suffix.lower()
    if suffix == ".evtx":
        yield from _iter_evtx_events(path)
    else:
        yield from _iter_json_events(path)

def _load_sigma_title_map(rules_dir: Path) -> Dict[str, str]:
    title_map: Dict[str, str] = {}
    if not rules_dir or not rules_dir.exists():
        return title_map
    for p in rules_dir.rglob("*"):
        if not p.is_file() or p.suffix.lower() not in {".yml", ".yaml"}:
            continue
        try:
            data = yaml.safe_load(p.read_text(encoding="utf-8", errors="ignore"))
        except Exception:
            continue
        if not isinstance(data, dict):
            continue
        title = data.get("title")
        if isinstance(title, str) and title:
            key = title.strip()
            if key and key not in title_map:
                title_map[key] = p.name
            norm = key.lower()
            if norm and norm not in title_map:
                title_map[norm] = p.name
    return title_map

def _build_report(
    input_path: Path | None,
    yara_path: Path | None,
    sigma_path: Path | None,
    logs_path: Path | None,
    out_path: Path,
    compute_hash: bool,
    timeout: int,
) -> Dict[str, Any]:
    out_path.parent.mkdir(parents=True, exist_ok=True)
    report: Dict[str, Any] = {
        "meta": {
            "generated_at": datetime.now().isoformat(timespec="seconds"),
            "input": "uploaded" if input_path else None,
            "yara_rules": str(yara_path) if yara_path else None,
            "sigma_rules": str(sigma_path) if sigma_path else None,
            "sigma_logs": str(logs_path) if logs_path else None,
            "out_path": str(out_path),
            "zircolite": None,
        },
        "files": [],
        "sigma_hits": [],
    }

    yara_engine = YaraEngine(yara_path) if yara_path else None

    if input_path:
        file_list = list(iter_files(input_path, follow_symlinks=False))
        for fp in file_list:
            item: Dict[str, Any] = {
                "path": fp.name,
                "name": fp.name,
                "size": fp.stat().st_size if fp.exists() else None,
                "sha256": None,
                "yara": [],
                "is_malicious": 0,
            }
            if compute_hash:
                try:
                    item["sha256"] = sha256_file(fp)
                except Exception:
                    item["sha256"] = None

            if yara_engine:
                matches = yara_engine.scan_file(fp, timeout=timeout)
                item["yara"] = [
                    {
                        "rule": m.rule,
                        "namespace": m.namespace,
                        "rule_file": m.rule_file,
                        "tags": m.tags,
                        "meta": m.meta,
                    }
                    for m in matches
                ]
                if matches:
                    item["is_malicious"] = 1

            report["files"].append(item)

    if sigma_path and logs_path and logs_path.exists():
        zircolite_path = find_zircolite_path(BASE_DIR)
        if not zircolite_path:
            raise RuntimeError(
                "Zircolite not found. Set ZIRCOLITE_PATH or place tools/zircolite/zircolite.py."
            )
        zircolite_out = logs_path.parent / (out_path.stem + "_zircolite.json")
        exit_code, output = run_zircolite(
            zircolite_path=zircolite_path,
            log_path=logs_path,
            rules_path=sigma_path,
            output_path=zircolite_out,
            python_exe=os.environ.get("PYTHON_EXE", sys.executable),
        )
        report["meta"]["zircolite"] = {
            "path": str(zircolite_path),
            "exit_code": exit_code,
        }
        if output:
            report["meta"]["zircolite"]["message"] = output[:5000]
        if exit_code != 0:
            raise RuntimeError("Zircolite failed. Check report meta for details.")
        report["sigma_hits"] = parse_zircolite_hits(zircolite_out)
        title_map = _load_sigma_title_map(sigma_path)
        if title_map:
            for hit in report["sigma_hits"]:
                for h in hit.get("hits", []):
                    if not h.get("rule_file") and h.get("rule_title"):
                        key = str(h["rule_title"]).strip()
                        h["rule_file"] = title_map.get(key) or title_map.get(key.lower())

    out_path.write_text(json.dumps(report, ensure_ascii=False, indent=2), encoding="utf-8")
    _prune_reports(keep=5)
    return report


def _summarize_report(report: Dict[str, Any], elapsed: float) -> Dict[str, Any]:
    files = report.get("files", [])
    malicious = sum(1 for f in files if int(f.get("is_malicious", 0)) == 1)
    yara_hits = sum(len(f.get("yara", [])) for f in files)
    sigma_hits = len(report.get("sigma_hits", []))
    has_sigma_log = bool(report.get("meta", {}).get("sigma_logs"))
    scanned_total = len(files) + (1 if has_sigma_log else 0)
    malicious_total = malicious + (1 if sigma_hits > 0 else 0)
    return {
        "files_scanned": scanned_total,
        "malicious": malicious_total,
        "yara_hits": yara_hits,
        "sigma_hits": sigma_hits,
        "elapsed": f"{elapsed:.2f}s",
        "report_path": report.get("meta", {}).get("out_path"),
    }


def _file_rows(report: Dict[str, Any]) -> Tuple[List[Dict[str, Any]], bool]:
    rows: List[Dict[str, Any]] = []
    files = report.get("files", [])
    truncated = len(files) > FILE_LIMIT
    for item in files[:FILE_LIMIT]:
        matches = [m.get("rule") for m in item.get("yara", []) if m.get("rule")]
        rule_files = [m.get("rule_file") for m in item.get("yara", []) if m.get("rule_file")]
        match_display = ", ".join(matches[:4])
        if len(matches) > 4:
            match_display += f" +{len(matches) - 4} more"
        rows.append(
            {
                "path": item.get("path"),
                "size": item.get("size"),
                "sha256": item.get("sha256"),
                "is_malicious": bool(item.get("is_malicious", 0)),
                "yara_count": len(matches),
                "yara_rules": match_display,
                "yara_rule_files": ", ".join(sorted(set(rule_files))),
            }
        )
    return rows, truncated


def _sigma_rows(report: Dict[str, Any]) -> Tuple[List[Dict[str, Any]], bool]:
    rows: List[Dict[str, Any]] = []
    hits = report.get("sigma_hits", [])
    truncated = len(hits) > SIGMA_LIMIT
    preferred_keys = [
        "EventID",
        "Image",
        "CommandLine",
        "ParentImage",
        "User",
        "Computer",
        "Channel",
        "ProviderName",
    ]
    aggregate: Dict[tuple[str, str], Dict[str, Any]] = {}
    for item in hits[:SIGMA_LIMIT]:
        excerpt_dict = item.get("event_excerpt", {}) if isinstance(item.get("event_excerpt", {}), dict) else {}
        excerpt_parts: List[str] = []
        for key in preferred_keys:
            if key in excerpt_dict and excerpt_dict[key]:
                excerpt_parts.append(f"{key}={excerpt_dict[key]}")
        if not excerpt_parts:
            for key, value in list(excerpt_dict.items())[:4]:
                if value is not None and value != "":
                    excerpt_parts.append(f"{key}={value}")
        excerpt_text = " | ".join(excerpt_parts)
        if len(excerpt_text) > 220:
            excerpt_text = excerpt_text[:217] + "..."

        for hit in item.get("hits", []):
            rule_title = str(hit.get("rule_title") or "未知").strip()
            rule_file = str(hit.get("rule_file") or "").strip()
            key = (rule_title, rule_file)
            if key not in aggregate:
                aggregate[key] = {
                    "rules": rule_title or "未知",
                    "rule_files": rule_file,
                    "excerpt": excerpt_text or "-",
                    "count": 0,
                }
            aggregate[key]["count"] += 1

    for (rule_title, rule_file), info in aggregate.items():
        rows.append(
            {
                "count": info["count"],
                "rules": info["rules"],
                "rule_files": info["rule_files"] or "-",
                "excerpt": info["excerpt"],
                "malicious": "是",
            }
        )
    return rows, truncated


def _compute_metrics(report_path: Path, groundtruth_path: Path) -> Dict[str, Any]:
    report = json.loads(report_path.read_text(encoding="utf-8", errors="ignore"))
    y_pred: Dict[str, int] = {}
    for it in report.get("files", []):
        name = it.get("name")
        if name:
            y_pred[name] = int(it.get("is_malicious", 0))

    # If a Sigma log was scanned, include its result as a predicted sample
    # so that metrics can evaluate log-level detections. Use the log filename
    # as the key; mark 1 if any sigma_hits present, otherwise 0.
    try:
        sigma_log = report.get("meta", {}).get("sigma_logs")
        if sigma_log:
            sigma_key = Path(sigma_log).name
            y_pred.setdefault(sigma_key, 0)
            if report.get("sigma_hits"):
                # if there are any sigma hits, consider the log detected
                y_pred[sigma_key] = 1
    except Exception:
        pass

    y_true: Dict[str, int] = {}
    with groundtruth_path.open("r", encoding="utf-8", errors="ignore", newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            n = row.get("name") or row.get("file") or row.get("filename")
            lbl = row.get("label") or row.get("malicious")
            if n is None or lbl is None:
                continue
            y_true[str(n)] = int(lbl)

    c = compute_confusion(y_true, y_pred)
    total = c.tp + c.fp + c.fn + c.tn
    accuracy = (c.tp + c.tn) / total if total else 0.0
    fnr = c.fn / (c.fn + c.tp) if (c.fn + c.tp) else 0.0
    return {
        "accuracy": f"{accuracy:.2%}",
        "accuracy_pct": round(accuracy * 100, 1),
        "fpr": f"{c.fpr:.2%}",
        "fpr_pct": round(c.fpr * 100, 1),
        "fnr": f"{fnr:.2%}",
        "fnr_pct": round(fnr * 100, 1),
    }


def _safe_output_path(rel_path: str) -> Path:
    target = (OUTPUT_DIR / rel_path).resolve()
    output_root = OUTPUT_DIR.resolve()
    if output_root not in target.parents and target != output_root:
        raise ValueError("Invalid output path.")
    return target


@app.route("/", methods=["GET"])
def index() -> str:
    scan_id = request.args.get("scan_id")
    cached = _SCAN_CACHE.get(scan_id) if scan_id else None
    if scan_id and scan_id != _LATEST_SCAN_ID:
        cached = None
    return render_template(
        "index.html",
        scan_form=DEFAULT_SCAN_FORM.copy(),
        metrics_form=cached.get("metrics_form", {}) if cached else {},
        scan_result=cached.get("scan_result") if cached else None,
        metrics_result=cached.get("metrics_result") if cached else None,
        message=cached.get("message") if cached else None,
        error=cached.get("error") if cached else None,
    )


@app.route("/download/<path:rel_path>", methods=["GET"])
def download_report(rel_path: str):
    try:
        target = _safe_output_path(rel_path)
    except Exception:
        abort(404)
    if not target.exists() or not target.is_file():
        abort(404)
    return send_file(target, as_attachment=True)


@app.route("/scan", methods=["POST"])
def scan() -> str:
    scan_form = {
        "timeout": request.form.get("timeout", DEFAULT_SCAN_FORM["timeout"]),
        "hash": _parse_bool(request.form.get("hash")),
    }

    upload_root = Path(tempfile.mkdtemp(prefix="avscan_"))
    input_dir = upload_root / "input"
    logs_dir = upload_root / "logs"

    try:
        input_files = request.files.getlist("input_dir") + request.files.getlist("input_files")
        saved_inputs = _save_uploads(input_files, input_dir)

        yara_path = None
        if saved_inputs:
            default_yara = BASE_DIR / "rules" / "yara"
            yara_path = default_yara if default_yara.exists() else None
            if not yara_path:
                return render_template(
                    "index.html",
                    scan_form=scan_form,
                    metrics_form={},
                    scan_result=None,
                    metrics_result=None,
                    message=None,
                    error="YARA 规则未找到，请检查内置规则库。",
                )

        logs_file = request.files.get("sigma_log")
        logs_path = None
        if logs_file and logs_file.filename:
            saved_logs = _save_uploads([logs_file], logs_dir)
            logs_path = saved_logs[0] if saved_logs else None

        if not saved_inputs and not logs_path:
            return render_template(
                "index.html",
                scan_form=scan_form,
                metrics_form={},
                scan_result=None,
                metrics_result=None,
                message=None,
                error="请上传文件夹/文件或 Sigma 日志文件。",
            )

        sigma_path = None
        if logs_path:
            default_sigma = BASE_DIR / "rules" / "sigma"
            sigma_path = default_sigma if default_sigma.exists() else None
            if not sigma_path:
                return render_template(
                    "index.html",
                    scan_form=scan_form,
                    metrics_form={},
                    scan_result=None,
                    metrics_result=None,
                    message=None,
                    error="Sigma 规则未找到，请检查内置规则库。",
                )

        out_path = OUTPUT_DIR / f"web_report_{_job_id()}.json"
        timeout = _safe_int(scan_form.get("timeout"), 20)
        compute_hash = bool(scan_form.get("hash"))

        input_path = input_dir if saved_inputs else None

        start = time.time()
        try:
            report = _build_report(
                input_path=input_path,
                yara_path=yara_path,
                sigma_path=sigma_path,
                logs_path=logs_path,
                out_path=out_path,
                compute_hash=compute_hash,
                timeout=timeout,
            )
        except Exception as exc:
            return render_template(
                "index.html",
                scan_form=scan_form,
                metrics_form={},
                scan_result=None,
                metrics_result=None,
                message=None,
                error=f"Scan failed: {exc}",
            )

        elapsed = time.time() - start
        summary = _summarize_report(report, elapsed)
        summary["report_path"] = str(out_path)
        summary["download_url"] = url_for(
            "download_report",
            rel_path=str(out_path.relative_to(OUTPUT_DIR)).replace("\\", "/"),
        )
        file_rows, file_trunc = _file_rows(report)
        sigma_rows, sigma_trunc = _sigma_rows(report)
        yara_rule_files = sorted(
            {
                m.get("rule_file")
                for f in report.get("files", [])
                for m in f.get("yara", [])
                if m.get("rule_file")
            }
        )
        sigma_rule_files = sorted(
            {
                h.get("rule_file")
                for hit in report.get("sigma_hits", [])
                for h in hit.get("hits", [])
                if h.get("rule_file")
            }
        )
        yara_matched_files = sum(1 for f in report.get("files", []) if f.get("yara"))
        sigma_matched_events = len(report.get("sigma_hits", []))
        scan_result = {
            "summary": summary,
            "files": file_rows,
            "sigma_hits": sigma_rows,
            "file_truncated": file_trunc,
            "sigma_truncated": sigma_trunc,
            "yara_enabled": bool(saved_inputs),
            "sigma_enabled": bool(logs_path),
            "yara_rule_files": yara_rule_files,
            "sigma_rule_files": sigma_rule_files,
            "yara_matched_files": yara_matched_files,
            "sigma_matched_events": sigma_matched_events,
        }

        metrics_form = {
            "last_report": str(out_path.relative_to(OUTPUT_DIR)).replace("\\", "/"),
        }
        global _LATEST_SCAN_ID
        _SCAN_CACHE.clear()
        scan_id = uuid4().hex
        _LATEST_SCAN_ID = scan_id
        _SCAN_CACHE[scan_id] = {
            "metrics_form": metrics_form,
            "scan_result": scan_result,
            "metrics_result": None,
            "message": "Scan complete. Report saved.",
            "error": None,
        }
        return redirect(url_for("index", scan_id=scan_id))
    finally:
        shutil.rmtree(upload_root, ignore_errors=True)


@app.route("/metrics", methods=["POST"])

def metrics() -> str:
    last_report = request.form.get("last_report")
    report_path = None
    if last_report:
        try:
            report_path = _safe_output_path(last_report)
        except Exception:
            report_path = None

    default_gt = BASE_DIR / "samples" / "groundtruth.csv"
    groundtruth_path = default_gt if default_gt.exists() else None

    if not report_path or not report_path.exists():
        return render_template(
            "index.html",
            scan_form=DEFAULT_SCAN_FORM.copy(),
            metrics_form={},
            scan_result=None,
            metrics_result=None,
            message=None,
            error="Report not found. Run a scan first.",
        )
    if not groundtruth_path or not groundtruth_path.exists():
        return render_template(
            "index.html",
            scan_form=DEFAULT_SCAN_FORM.copy(),
            metrics_form={},
            scan_result=None,
            metrics_result=None,
            message=None,
            error="Groundtruth CSV not found. Put it at samples/groundtruth.csv.",
        )

    try:
        metrics_result = _compute_metrics(report_path, groundtruth_path)
    except Exception as exc:
        return render_template(
            "index.html",
            scan_form=DEFAULT_SCAN_FORM.copy(),
            metrics_form={},
            scan_result=None,
            metrics_result=None,
            message=None,
            error=f"Metrics failed: {exc}",
        )

    return render_template(
        "index.html",
        scan_form=DEFAULT_SCAN_FORM.copy(),
        metrics_form={},
        scan_result=None,
        metrics_result=metrics_result,
        message="Metrics computed.",
        error=None,
    )


if __name__ == "__main__":
    app.run(debug=True)
